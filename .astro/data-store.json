[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.15.1","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://curt-mitch.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null,null,null,null],\"rehypePlugins\":[null,null,null,[null,{\"defaultLanguage\":\"js\",\"ignoreMissing\":true}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","content-config-digest","29109e263077c552","blog",["Map",11,12,28,29,44,45,57,58,72,73,86,87,99,100,114,115,128,129,140,141,152,153,165,166,178,179,191,192,204,205],"concluding-springboard-next-adventure",{"id":11,"data":13,"body":23,"filePath":24,"digest":25,"legacyId":26,"deferredRender":27},{"title":14,"date":15,"tags":16,"draft":19,"summary":20,"authors":21},"Concluding my Springboard Journey, and Starting my Next Adventure","2020-11-27",[17,18],"milestone","career",false,"Wrapping up the ML engineering track at Springboard and beginning my job hunt",[22],"default","During this week of Thanksgiving I'm incredibly thankful to the staff and my fellow students at Springboard, as I finally completed the requirements for the machine learning engineering program. When I was included in a COVID-19-related layoff in May I was very fortunate in that just the week before I had been accepted to the ML engineering program, so I knew right away where to do with my newly open schedule. I'm also incredibly thankful to my wife for her support and grateful for the ability to spend several months focusing on completing the Springboard program and working on significant related side projects such as this website.\n\nThrough the ML engineering program I covered a huge range of topics from reviewing how linear regression works to methods for deploying full-scale ML applications in a production environment. I also completed my end-to-end capstone project that I deployed on [this site](/blog/jp-en-translator-walkthrough), and met several other Springboard students and mentors across multiple disciplines. I'm especially grateful to my mentor [Dhiraj Kumar](https://dhirajkumarblog.medium.com/), who assisted me through our weekly 1:1 calls by answering all of my questions about the ML engineering curriculum and providing guidance at several stages of my capstone project.\n\nNow that I've completed the program my search for a full-time ML engineering role begins in earnest. If you're looking to hire or know a team that is, I'd be extremely grateful if you could let me know! My email is curtis.l.mitchell AT gmail.com.","src/content/blog/concluding-springboard-next-adventure.mdx","6f4b4a06d6cfe95f","concluding-springboard-next-adventure.mdx",true,"evaluating-machine-translation-models",{"id":28,"data":30,"body":40,"filePath":41,"digest":42,"legacyId":43,"deferredRender":27},{"title":31,"date":32,"tags":33,"draft":19,"summary":38,"authors":39},"How to Evaluate a Machine Translation Model","2020-08-30",[34,35,36,37],"natural language processing","machine learning","machine translation","python","An overview of metrics for rating machine translation performance",[22],"Gauging how accurately a machine learning model performs is one of the key questions any ML practioner needs to answer during model development. For many types of tasks such as image recognition, evaluating how accurate a model's results are is relatively straightfoward: the image label is either correct or it isn't. But how do you measure how accurate a machine translation result is? After all, for any but the simplest ideas, it's usually possible to convey that idea in multiple ways in any language. In this post we'll examine a few of the most common NLP metrics and how they are computed when assessing the performance of machine translation models.\n\nAs I mentioned above, beyond the simplest of sentences multiple translations of one language to another are possible. For instance, the English sentence “Yesterday I went to the store” could be translated into Spanish as “Ayer yo me fui a la tienda.” But since Spanish allows the dropping of subjects, in this case “yo” (because listeners can infer the subject from the form of the verb's conjugation), this sentence could also be “Ayer me fui a la tienda.” Here is where confusion about the best translation can start to arise. For instance, for a native Spanish-speaker which sentence is closer to the original English sentence? And this is still a fairly simple example where the two languages are relatively closely related and the word order in each sentence is the same!\n\n# Building Blocks\n\nWith the advent of statistical machine translation in the last 20 years and the more recent rise of deep learning applications in NLP, researchers have created many metrics to “score” the accuracy of machine translation models. In order to calculate a numerical value these various metrics use a variety of measures and linguistic features in sentences and entire documents (in NLP, a document is usually referred to as a “corpus”, plural “corpuses”), primarily n-grams and precision/recall, which we'll look at here.\n\n## n-grams\n\nIn NLP n-grams, where “n” stands for any whole number, are the sets of “n” neighboring words in a text. A “unigram” or 1-gram would be any single word, “bigrams” or 2-grams any two neighboring words, and so forth. Below is a table of n-grams for the opening sentence of Nobel Prize-winning author Yasunari Kawabata's famous 1948 novel, Snow Country: “The train came out of the long tunnel into the snow country.”:\n\nunigrams: [“The”, “train”, “came”, “out”, “of, “the”, “long”, “tunnel”, “into”, “the”, ”snow”, “country”]\n\nbigrams: [“The train”, “train came”, “came out”, “out of”, “of the”, “the long”, “long tunnel”, “tunnel into”, “into the”, “the snow”, ”snow country”]\n\ntrigrams: [“The train came”, “train came out”, “came out of”, “out of the”, “of the long”, “the long tunnel”, “long tunnel into”, “tunnel into the”,\n“into the snow”, “the snow country”]\n\n4-grams: [“The train came out”, “train came out of”, “came out of the”, “out of the long”, “of the long tunnel”, “the long tunnel into”, “long tunnel into the”, “tunnel into the snow”, “into the snow country”]\n\n_note_: N-grams over 3 are typically written with the number, but the bigger the n-gram the less frequently it is used for most NLP tasks.\n\n## precision and recall\n\nPrecision is a measure of how well your model performs at detecting members of the class you're looking for. We use the total number of correctly and incorrectly labeled positives in the denominator to represent precision as:\n\n$$\n\\frac{true\\:positives}{true\\:positives + false\\:positives}\n$$\n\nRecall (also called “sensitivity”) is similar to precision, wherein the percentage of correct labels that were detected by your model is calculated. Because the total number of true positives and false negatives is the total number of examples, we use that as the denominator and thus recall can be represented as:\n\n$$\n\\frac{true\\:positives}{true\\:positives + false\\:negatives}\n$$\n\nNote that each metric discussed here uses a different type of average, the details of which I'll present towards the end of the post along with links to the original papers that introduced each metric.\n\n# BLEU\n\nThe BLEU score has a strong claim to be the most-well known translation metric, as it's usually the one all other metrics are compared to. It was proposed by Kishore Papineni and his coauthors in a 2002 paper at the Annual Meeting of the Association for Computational Linguistics (ACL). BLEU stands for “BiLingual Evaluation Understudy” and compares a candidate sentence (i.e., the machine-generated sentence in the case of NLP models) to reference sentences that are usually created by human translators. The general idea behind the BLEU score is that it compares the n-grams in both the candidate sentence and reference sentences and counts the number of matches. The positions of the n-grams within each sentence do not matter, and the more matches there are the higher the BLEU score is.\n\nLet's look at some examples to build our intuition. We'll compare a known reference sentence and a hypothesis sentence that's hypothetically the output of a machine translation program, and both sentences will be stored as arrays of tokens representing individual words. We can use the sentence-level BLEU score that comes as part of the NLTK API:\n\n```python\nfrom nltk.translate.bleu_score import sentence_bleu\nreference_1 = ['the', 'hungry', 'gray', 'dog', 'ate', 'the', 'tasty', 'treat']\nhypothesis_1 = ['the', 'hungry', 'gray', 'dog', 'ate', 'the', 'tasty', 'treat']\nscore = sentence_bleu([reference_1], hypothesis_1)\nscore # -> 1.0\n```\n\n_note_: you may notice that the argument for the reference sentence is inside list brackets. This is because the BLEU methods in NLTK assume there will generally be more than one reference sentence passed as a list.\n\nNot surprisingly, this method returns a perfect score when the two sentences are identical.\n\nWhat happens when we change one word in the hypothesis sentence (in this case, the second “the” becomes “a”)?\n\n```python\nhypothesis_2 = ['the', 'hungry', 'gray', 'dog', 'ate', 'a', 'tasty', 'treat']\nscore = sentence_bleu([reference_1], hypothesis_2)\nscore # -> 0.5946035575013605\n```\n\nWe can see a dramatic drop in the score here. It won't be intuitive based on the score value, but the BLEU score is calculated by comparing the precision of n-grams in the hypothesis and reference sentences. Another thing to note is that by default the BLEU methods in NTLK compare 1- to 4-grams. If we were to only compare unigrams and bigrams by specifying their weighting, our score would be significantly boosted:\n\n```python\nscore = sentence_bleu([reference_1], hypothesis_1, weights=(0.5, 0.5))\nscore # -> 0.7905694150420949\n```\n\nMore concretely, the BLEU score is comparing the precision and recall of the hypothesis sentence versus the reference sentences. Assuming we're only looking at the unigrams of each sentence, the BLEU score tests to see how many of the words in the reference sentence (8 words total) appear in the hypothesis sentence (7, since the second “the” has become “a”). This would give a score of 7/8 = 0.875.\n\nWhen computing the BLEU score for an entire corpus, the precision scores of individual sentences are combined using the geometric mean, along with a penalty applied when the length of the reference corpus is shorter is less than or equal the length of the hypothesis sentence.\n\nAlthough the BLEU score was innovative when introduced and creates scores between 0 and 1 that can be easy to reason about, compared to more recently-introduced methods it can seem somewhat simplistic to use a method that only compares the number of overlapping n-grams.\n\n# NIST\n\nIn 2002 DARPA (The US Defense Advanced Research Projects Agency) commissioned NIST (the US National Institute of Standards and Technology) to evaluate and create their own machine translation metric based on BLEU. The result was a method similar to the BLEU score but with a few minor changes such as increased weighting for less-common n-grams and a reduced scoring penalty for shorter sentences (known as the “brevity penalty”):\n\n```python\nfrom nltk.translate.nist_score import sentence_nist\nscore = sentence_nist([reference_1], hypothesis_1)\nscore # -> 3.0357142857142856\n```\n\nand with the second hypothesis sentence:\n\n```python\nscore = sentence_nist([reference_1], hypothesis_2)\nscore # -> 2.642857142857143\n```\n\nNotice that the NIST scores will tend to be greater than one. This is due to the differences between the NIST and BLEU methods given above, plus the fact that the NIST method uses the arithmetic mean of the n-gram overlaps while BLEU uses the geometric mean. See the final section below for details on the differences.\n\nAlthough the NIST does have slight improvements over BLEU due to the higher weighting of uncommon n-grams and the different calculation of the brevity penalty, those same features make the potential range of score values more difficult to reason about.\n\n# METEOR\n\nAnother popular translation evaluation metric is METEOR, which stands for “Metric for Evaluation of Translation with Explicit ORdering.” METEOR was designed explicitly to make up for some of the shortcomings of BLEU, including a focus on sentence-level translations as opposed to BLEU's focus on translations at the corpus level.\nAdditionally, METEOR uses the harmonic mean (described in the mathematical details section below) on unigram precision and recall with a greater weighting for recall.\n\nLet's compute the METEOR scores using the same example sentence we used for the BLEU examples, noting that the meteor_score method expects sentences as whole strings instead of an array of tokens as with the BLEU and NIST methods:\n\n```python\nfrom nltk.translate.meteor_score import meteor_score\nreference_1 = 'the hungry gray dog ate the tasty treat'\nhypothesis_1 = 'the hungry gray dog ate a tasty treat'\nscore = meteor_score([reference_1], hypothesis_1)\nscore # -> 0.840561224489796\n```\n\n_note_: The NLTK meteor_score method takes full sentences as strings (or a list of sentences in the case of reference sentences) for its arguments instead of a list of tokens.\n\nWe can see that the METEOR score is significantly higher here versus that of the BLEU score (it was about 0.5946) on the same pair of sentences. This reflects one of the goals of the METEOR metric to more closely mimic human judgement versus the BLEU score, as most people would agree that the sentences are overall very similiar, i.e. that changing “the” to “a” in the translation would be a minor error.\n\nBecause of the increased weighting for recall, and the use of the harmonic mean which works better than the arithmetic or geometric means for situations where an average of weights is desired, METEOR is a great choice for machine translation models that is relatively simple to calculate (versus more modern methods) and remains easy to reason about. This is especially true for sentence level translations and these are the reasons I chose to use the METEOR score when evaluating my own translation model that I'll discuss in a future post.\n\n# Conclusion\n\nAs we've seen from the variety of machine translation evaluation metrics, there's no single method for determining translation accuracy. Depending on which properties of a sentence or corpus are measured, such as unigram recall rate or the weighting of the rareness of particular n-grams, there are a variety of metrics to choose from. On the other hand each metric provides some calculation of common components of a translation, usually the overlap of n-grams using some type of average.\n\nTranslation metrics is still a field of active development, but it's not uncommon for newer methods to be compared to the three metrics described here. In fact Google recently announced a newly developed metric they call “BLEURT” ([Google AI Blog: Evaluating Natural Language Generation with BLEURT](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)), so it's clear that BLEU is still often considered the metric to beat!\n\n# Supplement: Mathematical Details\n\n_Arithmetic mean_: The NIST metric utilizes the arithmetic mean, which is the most common version of an average and is usually what is implied when someone uses the word “mean” on its own. The arithmetic mean is the sum of n numbers divided by n:\n\n$$\nA = \\frac{1}{n} \\sum_{i-1}^n x_i = \\frac{x_1 + x_2 + ... + x_n}{n}\n$$\n\nFor instance, the arithmetic mean of 2, 4, and 3 would be $\\frac{2 + 4 + 3}{3} = \\frac{9}{3} = 3$\n\n_Geometric mean_: The BLEU metric utilizes the geometric mean of the number of overlapping n-grams. The geometric mean is defined as the nth root of the product of n numbers:\n\n$$\nG = (\\prod_{i=1}^n x_i)^\\frac{1}{n} = \\sqrt[n]{x_1 x_2 ... x_n}\n$$\n\nFor instance, the geometric mean of the numbers 1, 4, and 2 would be $(1 * 4 * 2)^\\frac{1}{3} = \\sqrt[3]{8} = 2$\n\n_Harmonic mean_: The METEOR metric utilizes the harmonic mean of the number of overlapping n-grams. The harmonic mean is defined as the reciprocal of the arithmetic mean of the reciprocals of n numbers:\n\n$$\nH = (\\frac{\\sum_{i=1}^n x_i^{-1}}{n})^{-1} = \\frac{n}{\\sum_{i=1}^n \\frac{1}{x_i}} = \\frac{n}{\\frac{1}{x_1} + \\frac{1}{x_2} + ... + \\frac{1}{x_n}}\n$$\n\nFor instance, the harmonic mean of 1, 4, and 4 would be $(\\frac{\\frac{1}{1} + \\frac{1}{4} + \\frac{1}{4}}{3})^{-1} = \\frac{3}{\\frac{1}{1} + \\frac{1}{4} + \\frac{1}{4}} = \\frac{3}{1.5} = 2$\n\n# Supplement: Original Papers for Machine Translation Metrics\n\nBLEU: [BLEU: a Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf)\n\nNIST: [Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics](https://dl.acm.org/doi/pdf/10.5555/1289189.1289273)\n\nMETEOR: [Meteor: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments](https://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf)","src/content/blog/evaluating-machine-translation-models.mdx","64838b98c07fca76","evaluating-machine-translation-models.mdx","beginning-your-open-source-journey",{"id":44,"data":46,"body":53,"filePath":54,"digest":55,"legacyId":56,"deferredRender":27},{"title":47,"date":48,"tags":49,"draft":19,"summary":51,"authors":52},"Beginning Your Open Source Journey","2023-10-28",[50,18],"open source","How I joined an open source project, and how you can too",[22],"One of the most rewarding things I've been able to do in my career is answer questions from people curious about transitioning to a career in tech or from those who are early in the transition and are still searching for their first job. A consistent piece of advice I offer, especially to the folks who have already made the tech transition, is to look for an open-source (OS) project to contribute to. People are often intimidated by contributing to an OS project, but it's really not as scary as it might seem, and it can be a great signal to employers on your competence to work with other people in a large project or codebase. And if you find the right project and community, it can turn into a fun hobby and even (as it was in my case) a path to a future job!\n\nIn this post I'd like to share some examples of my own path into an open-source community. One of the great things about OS is that by its very nature most everything is open and available to view on the internet.\n\n## Joining OpenMined\n\nAt the height of the COVID-19 pandemic in May 2020 I was laid off from the startup I had been at for the last few years. I had been trying to transition to machine learning engineering for a while, and fortunately I had just been accepted into Springboard's [ML engineering program](https://www.springboard.com/courses/ai-machine-learning-career-track/). As I progressed through the course material, I learned about the sub-field of privacy-preserving machine learning, or PPML. Initially my interest in this branch of ML was intellectual as the methods that blend cryptography with machine learning techniques are technically fascinating. However I also quickly began to appreciate how PPML could help address a host of social issues inherent is standard ML and AI practices, such as training datasets that can expose the privacy of individuals by including sensitive data such as medical and financial information.\n\nWhen I began researching groups working on PPML, I quickly came across the group [OpenMined](https://www.openmined.org/). They are a non-profit open-source group dedicated to building PPML software, and after a quick glance of their website I decided to join their Slack group and enroll in a [course](https://courses.openmined.org/courses/our-privacy-opportunity) they had just launched on privacy and technology.\n\n## My First Contribution\n\nAt one point while working through the course contents I noticed that a radio button on one part of the course web app wasn't working properly, so I opened an \"issues\" ticket and documented the bug: [https://github.com/OpenMined/openmined/issues/92](https://github.com/OpenMined/openmined/issues/92). Initially I thought that pointing out this issue was sufficient, and having been a developer for several years at this point I knew that OpenMined's developers would appreciate a well-documented bug filing. However I kept ruminating on possible causes for the issue, and a few days later I decided to start working on the problem myself. I asked the project lead if I could be assigned the work, which you can see in the comments on that ticket. Once I implemented a solution - a missing property on a React component - I opened a pull-request: [https://github.com/OpenMined/openmined/pull/94](https://github.com/OpenMined/openmined/pull/94), and I made sure to adhere to all of the suggested information in their pull-request template such as documenting how I had tested the code change.\n\nAfter the bug fix was approved and merged, the project lead asked if I'd be interested in contributing more since they were short-staffed (a common thing for open-source groups and non-profits). After passing a brief phone screen I began joining weekly calls with the frontend engineers working on their course material and volunteering a few hours a week with them.\n\n## From Open Source to a New Job Opportunity\n\nWithin a few months I scaled back my volunteering with OpenMined as I was busy moving houses and then began a new job contracting with NASA, but I kept following OpenMined's work and reading posts in the Slack group. Fortunately I was able to resume working with them a year later when I was accepted into the initial round of their padawan program to dive deeper into privacy-enhancing technologies (PETs) and the PySyft codebase. I've been working with them ever since! And becoming knowledgable of PETs and PySyft directly led to my current role as an [emerging technology fellow](https://www.xd.gov/news/announcing-2023-xd-emerging-technology-fellows/) at the Census Bureau. One of the key points in my favor during the interview process for this role was that I was able to demonstrate an example of my PETs knowledge with a pull-request on Github for a function to compute a differentially private mean: [https://github.com/OpenMined/PySyft/pull/6634](https://github.com/OpenMined/PySyft/pull/6634)[^1]\n\n## Simpler Paths to OS Contributions\n\nAdmittedly I had been working for several years as a web developer by the time I began contributing to OpenMined's projects, so I felt comfortable diagnosing and fixing the issue with the React component in the courses codebase. But there are plenty of simpler contributions that could be less intimidating to first-time OS contributors such as fixing or adding to project documentation.\n\nAs another personal example, when I was first learning about differential privacy I started reading the great book ([Programming Differential Privacy](https://programming-dp.com/)). It's an open source book that's entirely hosted on Github, so when I came across some small misspellings I opened pull-requests to get those fixed:\n\n- [https://github.com/uvm-plaid/programming-dp/pull/37](https://github.com/uvm-plaid/programming-dp/pull/37)\n- [https://github.com/uvm-plaid/programming-dp/pull/38](https://github.com/uvm-plaid/programming-dp/pull/38)\n\n## Beginning Your OS Journey\n\nHopefully now the idea of contributing to open source seems less intimidating. I often tell people that there is an OS project for any community and interest, and that the OS communities are looking for people with all skills sets, not just software developers. And while knowledge of git used to be a prerequisite for OS contributions, the user interfaces on platforms like Github and GitLab are constantly improving to lower the barrier to making proposed changes and opening pull requests.\n\nHopefully this post has sufficiently motivated you to begin your open source journey, and if so I encourage you to find a project of interest! Some quick googling and searches or posting a question on Reddit can often help you find some great codebases. As a newcomer I would look for an active codebase (ideally changes are made on a regular basis with the most recent occurring in the last few days or weeks) with some issues marked as “beginner friendly”, “good for first timers”, or something similar. For example, here are the [“Good first issue” tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aissue+is%3Aopen+label%3A%22Good+first+issue+%3Amortar_board%3A%22) in OpenMined's PySyft repository.\n\nIf you're a developer who is somewhat comfortable with git, I also suggest reading / watching introductions to how to fork repositories and open pull-requests. Here are just two quick examples but there are lots of great resources just a quick search away:\n\n- [My first contribution to open source: Make a fork of the repo](https://opensource.com/article/19/11/first-open-source-contribution-fork-clone)\n- [An Easy Introduction to Open Source, Github Cloning, Forking and Pull Requests](https://www.youtube.com/watch?v=xdHfqG_nlKg)\n\nBest of luck in your open-source journey, and don't hesitate to contact me if you have questions!\n\n[^1]: Experienced readers may note that this PR was closed and not actually merged, however the changes were included as part of a separate, larger PR of related features: [https://github.com/OpenMined/PySyft/pull/6884](https://github.com/OpenMined/PySyft/pull/6884), as referenced in [this comment](https://github.com/OpenMined/PySyft/pull/6634#issuecomment-1303798787) on the original PR.","src/content/blog/beginning-your-open-source-journey.mdx","4c34c739c4d60009","beginning-your-open-source-journey.mdx","frontend-migration-note",{"id":57,"data":59,"body":68,"filePath":69,"digest":70,"legacyId":71,"deferredRender":27},{"title":60,"date":61,"tags":62,"draft":19,"summary":66,"authors":67},"Upcoming Website Updates","2021-11-12",[63,64,65],"website","javascript","next-js","Migrating the frontend of this website and other upcoming updates",[22],"I'm closing in on one year since I've written a new piece for this site, but I wanted to leave one now to avoid the impression I'm no longer maintaining this project. Part of the reason has been a recent move and starting my current role at [NASA Ames](https://www.nasa.gov/ames/) back in May. More recently I've been busy working on refactoring parts of the site, primarily on the frontend. When I first created the site I didn't spend much of my effort on the client-side code. At the time I was enrolled in Springboard's [machine learning engineering program](https://www.springboard.com/courses/ai-machine-learning-career-track/), and since I was spending a lot of my time working through that curriculum I wanted to focus on this site's content and to a lesser extent getting the backend up and running (the backend is built on [Django](https://www.djangoproject.com/), which I hadn't previously worked with).\n\nIn any case it's now time for an upgrade. There's nothing obviously wrong with the site as it is, but there are some additional optimizations I could make and newer React patterns and best practices I'm keen on incorporating. First and foremost, I'm migrating the frontend framework I'm using from [Create React App](https://create-react-app.dev/) to [Next.js](https://nextjs.org/). Next is also a React-based framework, but it incorporates several new features I haven't previously used such as server-side rendering and asset optimizations that should improve loading and rendering performance. Additionally, I'll be switching from standard JavaScript to [TypeScript](https://www.typescriptlang.org/) for better type safety, updating [Material-UI](https://mui.com/) to utilize server-side rendering, and incorporating modern React patterns like hooks and functional components. I have been attempting to perform this migration more directly, but after several cumulative hours of troubleshooting, reading documentation and Stackoverflow, and banging my head against the wall I've decided instead to wholly rebuild the frontend as a new Next app. Dealing with various build and undefined errors got frustrating and since this is a side-project I feel it should be a mostly enjoyable experience! Once the transition is complete the content should all be the same, but the final UI might look slightly different. And once that's completed (hopefully by New Year's!) I have a lot of interesting posts planned such as finally writing a walkthrough of the Urdu number classifier (_note_: this app is not currently available) and a post about the history of cryptography.\n\nStay tuned!","src/content/blog/frontend-migration-note.mdx","b2202c6cd042e541","frontend-migration-note.mdx","introduction-to-zkps",{"id":72,"data":74,"body":82,"filePath":83,"digest":84,"legacyId":85,"deferredRender":27},{"title":75,"date":76,"tags":77,"draft":19,"summary":80,"authors":81},"An Introduction to Zero-Knowledge Proofs","2024-05-11",[78,79],"zero-knowledge proofs","privacy-enhancing technologies","A brief technical introduction to zero-knowledge proofs using the Feige-Fiat-Shamir identification scheme",[22],"What if you wanted to prove that you know some piece of information without revealing that information directly? For example, what if you could verify that the amount of money in your bank account meets some threshold (say, for a credit check) without having to disclose the exact amount of money you have? This is the promise of zero-knowledge proofs (ZKPs), a class of privacy-enhancing technology (PET) that allow someone to show they know some value or piece of information without having to divulge that information directly.\n\nZKPs are not a new idea, but they have experienced a recent growth in popularity, especially for applications in Web 3.0 and cryptocurrency domains. In this post I'll give a sense for how ZKPs work by walking through the process for one of the earliest examples of ZKPs, the [Feige-Fiat-Shamir identification scheme](https://en.wikipedia.org/wiki/Feige%E2%80%93Fiat%E2%80%93Shamir_identification_scheme). Note that the scheme relies on modular and exponential arithmetic, so familiarity with these concepts will be helpful.\n\n## Introducting the Cast\n\nSimilar to cryptocurrencies, many of the ideas in ZKPs originated in crypography. Discussions of cryptographic protocols usually involve \"Alice\" and \"Bob\" to represent two people sharing information, and another person, \"Eve\", attempting to eavesdrop on their communication.\n\nFor ZKP examples, instead of these three individuals we usually have the characters \"Peggy,\" who is attempting to prove she knows a piece of information without revealing it directly (also known as the \"Prover\"), and \"Victor\", who is attempting to verify Peggy's claim (also known as the \"Verifier\").\n\n## The Feige-Fiat-Shamir Identification Scheme\n\nThis scheme was originally introduced[^1] in 1988 by cryptographers [Uriel Feige](https://en.wikipedia.org/wiki/Uriel_Feige), [Amos Fiat](https://en.wikipedia.org/wiki/Amos_Fiat), and [Adi Shamir](https://en.wikipedia.org/wiki/Adi_Shamir). The essense of the scheme is that Peggy will prove to Victor that she knows some collection of numbers without sharing those numbers directly. Victor will challenge Peggy by picking random zeros and ones, and then a series of modular and exponential arithmetic steps are worked through that can only be solved with Peggy's secret numbers. This series of steps can be repeated any number of times until Victor is satisfied Peggy does know her numbers (and that she hasn't, for example, selected them at random).\n\nThroughout this description we'll be using Python-based coding examples. The only extra dependency we'll need is [SymPy](https://www.sympy.org/en/index.html) for its useful [`isprime`](https://docs.sympy.org/latest/modules/ntheory.html#sympy.ntheory.primetest.isprime) method.\n\n### Step 1: Generation of Random Primes and Peggy's Selection of Secret Values\n\nThroughout this example, we'll use numbers between 0 and 1000, much smaller numbers than are used in practice[^2], to make the process a bit easier to follow.\n\n```python\n# initializing prime list\nfrom sympy.ntheory import isprime\n\nmin_value = 1\nmax_value = 1000\nsmall_primes = [i for i in range(min_value, max_value) if isprime(i)]\n```\n\nNow we select two random prime numbers from this list, `p` and `q`, and multiply them to create a third number `n = p * q`.\n\n```python\nimport random\n\np, q = random.sample(small_primes, k=2)\nn = p * q\n```\n\nNext, Peggy creates her secret numbers. These can be any amount of secret numbers, but for this example we'll randomly select three that we'll call `s1`, `s2`, and `s3`. Importantly, for the arithmetic to work out these numbers must be coprime to `n` and they therefore cannot equal either `p` or `q`:\n\n```python\ns1, s2, s3 = random.sample([i for i in range(min_value, max_value) if i != p and i != q], k=3)\n```\n\nLastly, Peggy also generates a random value `r` and computes a value `x` that \"hides\" the true value of `r` by squaring `r` and taking its modulous with `n`:\n\n```python\nr = random.randint(min_value, max_value)\nx = (r**2) % n\n```\n\n### Step 2: Victor Challenges Peggy\n\nFor his part, Victor selects a random collection of 0's and 1's that we'll save as `a1`, `a2`, and `a3` meant to challenge Peggy's knowledge of the secret `s` values:\n\n```python\na1, a2, a3 = random.choices([0, 1], k=3)\n```\n\nVictor then sends his challenge values to Peggy while she, in turn, sends him the value of `x`.\n\nPeggy calculates a new value `y` that involves the product of the secret `s` values raised to the power of Victor's `a` challenge values, which is also multiplied by `r` and modulated by `n`.\n\n```python\ny = (r * ((s1**a1) * (s2**a2) * (s3**a3)) ) % n\n```\n\nPeggy also computes values `v` that square the secret `s` values and modulate them with `n`:\n```python\nv1 = (s1**2) % n\nv2 = (s2**2) % n\nv3 = (s3**2) % n\n```\n\nThe `v` values as well as `y` are then shared with Victor.\n\nLet's recap where we are at this point: Victor has a random selection of 0's and 1's that he shared with Peggy. Peggy has shared `x`, `y`, and the `v` and `a` values with Victor, but crucially she has not shared the secret values `s` nor the values `n` and `r`. In other words, Peggy has not shared the values she wants to prove she knows, but she has shared a number of values calculated (using modular and exponential arithmetic) with those secret numbers as well as Victor's challenge numbers, but all of the values Peggy has shared with Victor _are_ numbers that were computed from `n`, `r`, and the secret values.\n\nThe next step is to prove that these computed values could only have been created if Peggy indeed knows the values of `s`.\n\n### Step 3: Peggy Proves her Knowledge of the Secret Values\n\nThis step involves one more calculation. Victor confirms that the value `y` shared by Peggy is equal to the product of `x` and the values of `v` raised to the power of his `a` values, all modulo `n`:\n\n```python\nproof = (y**2 % n == (x * ( (v1**a1) * (v2**a2) * (v3**a3)) ) % n)\nprint(proof) # True\n```\n\n### Mathematical Review\n\nLet's review the definitions and equalities that support this proof. We'll go backwards through the steps we took above and use some mathematical notation rather than Python code this time:\n\n1. By the definition of `y` our proof takes the form\n$$\nmod(y^2, n) = mod((r^2 * ({s_1}^{a_1}) * ({s_2}^{a_2}) * ({s_3}^{a_3}) )^2, n)\n$$\n\nFrom here we'll remove the `mod` operator for clarity.\n\n2. By the definition of `x` we can replace the squared `r` term like so:\n$$\nr^2 * (({s_1}^{a_1}) * ({s_2}^{a_2}) * ({s_3}^{a_3}) )^2 = x * (({s_1}^{a_1}) * ({s_2}^{a_2}) * ({s_3}^{a_3}))^2\n$$\n\n3. If we expand the squared terms to the right of `x`, we get\n$$\nx * (({s_1}^{a_1}) * ({s_2}^{a_2}) * ({s_3}^{a_3}))^2 = x * ({s_1}^{a_1}) * ({s_1}^{a_1}) * ({s_2}^{a_2}) * ({s_2}^{a_2}) * ({s_3}^{a_3}) * ({s_3}^{a_3})\n$$\n\n4. Then applying the definition of the `v` terms, we get\n$$\nx * ({s_1}^{a_1}) * ({s_1}^{a_1}) * ({s_2}^{a_2}) * ({s_2}^{a_2}) * ({s_3}^{a_3}) * ({s_3}^{a_3}) = x * (({v_1}^{a_1}) * ({v_2}^{a_2}) * ({v_3}^{a_3}))\n$$\n\nThus returning us to the proof statement (and with the `mod` operator re-applied):\n$$\nmod(x * (({v_1}^{a_1}) * ({v_2}^{a_2}) * ({v_3}^{a_3})), n) = mod(y^2, n)\n$$\n\nThese steps confirm that Peggy indeed knows the secret `s` values and can prove it without needing to directly reveal those values.\n\nThere is a chance that Peggy could randomly generate a value for `y` such that the final step of the proof would be true without her actually knowing the secret values. And as mentioned before, the scheme designers propose doing several rounds of verification using different values for `r` and Victor's `a` challenge values where each successive and successful round would reduce the probability that Peggy is able to continually generate random values for `y` that also satisfy unknown values for the `s` secret values.\n\n## A Jumping Off Point for ZKPs\n\nThe Feige-Fiat-Shamir identification Scheme is a simple introduction to zero-knowledge proofs. The scheme is not used in practice due to its vulnerability to certain attacks, but it does demonstrate the core concepts of ZKPs such as proving knowledge of a value without revealing the value directly. More modern and robust ZKPs such as zk-SNARKs and zk-STARKs (types of [non-interactive zero-knowledge proofs](https://en.wikipedia.org/wiki/Non-interactive_zero-knowledge_proof)) are gaining adoption today, especially in the realm of cryptocurrencies.\n\nIf you're interested in learning more, I suggest checking out [ZKProof](https://zkproof.org/), an organization committed to establishing global standards for ZKPs. They have a great [list of resources](https://docs.zkproof.org/) to get you started. This Awesome Zero-Knowledge Proofs [GitHub repository](https://github.com/sCrypt-Inc/awesome-zero-knowledge-proofs) is also a great collection of links and resources worth exploring.\n\n[^1]: Original Paper: https://link.springer.com/article/10.1007/BF02351717\n\n[^2]: I have a previous post on the [size of cryptographic primes](/blog/how-big-are-cryptographic-primes) if you're curious how large these numbers might be in practice.","src/content/blog/introduction-to-zkps.mdx","54b187447878ccd1","introduction-to-zkps.mdx","how-to-ask-for-help-in-oss-communities",{"id":86,"data":88,"body":95,"filePath":96,"digest":97,"legacyId":98,"deferredRender":27},{"title":89,"date":90,"tags":91,"draft":19,"summary":93,"authors":94},"How to Ask for Help in Open-Source Communities","2025-5-26",[50,92],"debugging","Effective strategies for seeking help with open-source software",[22],"As someone who's been contributing to open-source software (OSS) projects periodically for over 5 years, I love to see OSS projects and communities with lots of users and frequent interactions. In addition to collaborating on Github or Gitlab, many of these communities make use of other platforms for easier communication, frequently Slack or Discord, which often include a channel for Q&A or helping users debug issues with the open-source software they're using.\n\nI've noticed some recurring patterns in the ways people ask for help in these channels, and in this post I'll share some suggestions to both get the most help on your questions and also increase the chances that you'll solve the issue on your own. Going through these steps has the added benefit of improving your software development and debugging skills!\n\n## 1. Share the text of the error message rather than a screenshot\nA common type of help post I see is someone writing a brief message that lacks details on the problem they're experiencing, and the post commonly includes a screenshot of the error message and/or stacktrace[^1]. What is more helpful than a screenshot is a copy/paste of the text of the error message and stacktrace. This makes it easier for the OSS maintainers to read the error text and allows them to copy/paste the text to perform their own web searches, or search the OSS codebase for the error messages and names of functions in the stack trace.\n\n## 2. Provide details on your environment\nAlong with including the text of error messages, it's often beneficial to describe the environment you're trying to run the OSS in. At a minimum, you should include the following information:\n* The hardware (type of personal computer or server), operating system, and operating system version number\n* The release or version number of the OSS you're using and seeking help with\n* Details on any virtual environment, package manager, etc., being used (for example, maybe the person seeking help is using Anaconda for Python development, or running experiments within a Jupyter notebook)\nIt can also be useful to describe what you're trying to do with the OSS, such as install it, use it within a larger program, etc.\n\n## 3. Describe what you've tried so far\nLastly, you'll be more likely to get help if you can describe what you've tried so far to fix the issue. When interacting with senior colleagues in a professional setting, \"What have you tried so far?\" is often the first reponse you'll hear when you ask them for help to debug an issue. It can be useful if you describe what you've attempted to resolve the problem on your own, including:\n* What have you searched for online via Google or other search engines\n* Exchanges you've had with AI/LLM systems to debug the issue\n* Attempts to rerun a command or process with different arguments, configurations, etc.\n\nThe great thing about detailing what you've tried is that it can frequently help you fix the problem on your own. This will especially be the case as you gain more experience. In my own career I can recall several times when I was stuck on an issue, and I began writing a message into Slack or Discord to ask for help. But in the process of writing the details of what the issue was and what I had tried so far to resolve it, I uncovered additional approaches to solving the problem that I hadn't tried yet. Frequently one of these new approaches would help me fix the issue![^2]\n\n## Helping OSS Devs and Yourself\nTo summarize, when you create a post asking OSS maintainers or the broader community for help, include the following information:\n1. the text of the error message and stacktrace (rather than a screenshot)\n2. the details of your environment and how you're using the OSS\n3. a brief description of what you've tried so far to resolve the issue\n\nUsing the above tactics will help get you unstuck faster. You'll also be helping the OSS community more effectively find and fix bugs, and by using this process you'll be improving your own debugging skills to become a more effective software professional!\n\n[^1]: A stacktrace is the list of messages a program will display when an error or failure occurs. It consists of several function calls (that's the \"stack\" part of the term) meant to help developers \"trace\" a problem to its source.\n[^2]: See \"Rubber Duck Debugging\" for a more thorough description of this method of talking through a software problem with someone (or something): https://rubberduckdebugging.com/.","src/content/blog/how-to-ask-for-help-in-oss-communities.mdx","f4585f7050e93beb","how-to-ask-for-help-in-oss-communities.mdx","how-big-are-cryptographic-primes",{"id":99,"data":101,"body":110,"filePath":111,"digest":112,"legacyId":113,"deferredRender":27},{"title":102,"date":103,"tags":104,"draft":19,"summary":108,"authors":109},"How Big Are the Prime Numbers Used in Cryptography?","2023-08-22",[105,106,107],"cryptography","mathematics","number theory","A look at the size and characteristics of cryptographic primes",[22],"During the height of the pandemic when I was between jobs, I attended a webinar on cryptography principles given by a security engineer. While the speaker was detailing how public-private key cryptography works, she described the primes used to create these keys as “very big”. When I raised my Zoom hand to ask if she could describe how big these primes are, the speaker said something like “several hundred digits.” I've thought about that answer off-and-on since then, and recently resolved to find a more concrete answer. I'll first answer the question directly and then explore a few natural followup questions including why they are the size they are, how these numbers are found, and exactly how many of these primes there are available to use in modern cryptography.\n\nThe short answer is that the exact size depends on the cryptographic algorithm being used, but the general [best practice as of 2023](https://weakdh.org/) is to use a 2048-bit prime number for methods like the Diffie-Hellman key exchange algorithm. To give a visual representation of that, here is a (approximately) 2048-bit number derived using Python's cryptographic [`getPrime`](https://pycryptodome.readthedocs.io/en/latest/src/util/util.html#Crypto.Util.number.getPrime) method:\n\n\u003CLargePrime />\n\nThis number is 617 digits long. That length is based on the fact that we've converted a 2048-bit binary number (“bit” is short for “binary digit”) to a decimal (base 10) digit. To get from a value of 2,048 binary digits, we will have at most $kn$ decimal digits, where $k = log_{10} 2$, or about 0.301, and $n$ is the number of binary digits. Thus, 2048 times 0.301 rounded up to the nearest whole integer gives us 617.\n\n## How do we find these numbers?\n\nIt might seem intuitive that these numbers would be picked out of a list. After all, having to “find” a new large prime number every time a cryptographic process is established seems like a lot of work… but that's exactly what happens! The general approach is to generate an odd random number of the appropriate size (617 digits for a base-10 number in the case above), and then run some tests to see if it's a prime number.\n\nThe first of these tests is to divide that number by a list of the first few hundred prime numbers. If any of those “small” primes can evenly divide the randomly-generated candidate number, then we know it's a composite number (a number with multiple factors besides itself and 1), not a prime, and the entire process restarts with a new, randomly-generated number. If the candidate number cannot be evenly divided by any of these smaller primes, we move onto a second, more complicated and probabilistic test that doesn't tell us with 100% certainty that a number is a large prime, but can get us within an acceptable level of confidence that we have found a large prime number. In practice it's usually acceptable to find a number that is _probably_ prime because it would still be difficult to find factors for such a large number, and because most modern cryptographic processes that use these large numbers will find new numbers after some brief amount of time, usually on the order of a few minutes to several hours.\n\nFor the second test to determine if our candidate is highly likely to be prime, there are [several tests](https://en.wikipedia.org/wiki/List_of_number_theory_topics#Primality_tests) available. These tests rely heavily on [modular arithmetic](https://en.wikipedia.org/wiki/Modular_arithmetic) and properties such as [congruence](https://en.wikipedia.org/wiki/Congruence_relation), in which more than one number have the same remainder when divided by a particular integer. The [Miller-Rabin Test](https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test) is the most widely used of these tests, and it works by selecting a random number between 2 and 1 minus our candidate prime, and then checking if that number is a divisor of our candidate number by running through one or more calculations of congruence relations. It does these checks in a relatively fast manner and to a high degree of certainty compared to other primality tests, hence its popularity in modern cryptographic protocols. The details of the test's calculations are beyond the scope of this post, but I have some links to more detailed resources below.\n\nTo describe our algorithm for finding large prime numbers from a high level, here's what steps we take:\n\n1. Pick a random number of appropriate size (often 2048 bits)\n2. Test for primeness using a low-level primality test (i.e., divide by list of smaller primes)\n3. Test for primeness using a high-level primality test such as the Miller-Rabin Test\n\n## How many of these prime numbers exist?\n\nIn the last section on finding a large prime we wondered whether we could pull these numbers from a pre-existing list to save us the trouble of finding a new number every time we need one. A natural followup question might be to ask how many of these large prime numbers exist. After all, what if someone has a large enough computer to store a list of these numbers to check against? That would effectively nullify this entire process!\n\nFortunately for modern cryptography and all of the technologies that rely on it, it is physically impossible to store all of these large, 2048-bit prime numbers. Thanks to the [prime number theorem](https://en.wikipedia.org/wiki/Prime_number_theorem), we can roughly (but still quantitatively) determine the distribution of prime numbers amongst a range of all integers using the formula $π(N) \\sim \\frac{N}{ln(N)}$, which also gives us the count of prime numbers up to the number $N$ using the [prime-counting function](https://en.wikipedia.org/wiki/Prime-counting_function) $π(N)$. Using it to find the number of primes of exactly 2048 bits, let alone primes of other sizes, gives us more than $10^{600}$. This is significantly greater than the estimate for the number of atoms in the known universe, which is currently around $10^{80}$, and hence storing all of these numbers would be against the laws of physics. Such a gigantic amount of possible prime numbers also makes collisions (i.e., creating the same prime number more than once) effectively impossible. Finding one specific prime to break a cryptographic protocol such as the RSA algorithm is hopeless for even the most powerful supercomputers, even if the protocols that use these prime numbers weren't switching to new large primes every few minutes or hours.\n\n## Summary\n\nHopefully now we have a basic understanding of why prime numbers used in modern cryptographic protocols are the size they are, how we derive and test those prime numbers, and a sense for how many of these numbers there are available. An even more basic question might be to ask what makes prime numbers so special to use in cryptography in the first place. But that will have to be the subject of a future post. Stay tuned!\n\n## Additional Links\n\n[Cryptographic Key Length Recommendations](https://www.keylength.com/)\n\nCryptography Stack Exchange: [How can I generate large prime numbers for RSA?](https://crypto.stackexchange.com/questions/71/how-can-i-generate-large-prime-numbers-for-rsa)\n\nMedium: [Finding Large Primes for Public Key Cryptography - The Miller-Rabin primality test](https://ghenshaw-work.medium.com/finding-large-primes-for-public-key-cryptography-9c5a5c0d32c4)\n\nPaper: [Is there a shortage of primes for cryptography?](https://homes.cerias.purdue.edu/~ssw/shortage.pdf)\n\n[How Many Primes Are There?](https://t5k.org/howmany.html) - Good description of the Prime Number Theorem","src/content/blog/how-big-are-cryptographic-primes.mdx","40f945f56533f07d","how-big-are-cryptographic-primes.mdx","jp-en-translator-walkthrough",{"id":114,"data":116,"body":124,"filePath":125,"digest":126,"legacyId":127,"deferredRender":27},{"title":117,"date":118,"tags":119,"draft":19,"summary":122,"authors":123},"Creating A Japanese-English Translation Application","2020-11-09",[34,36,35,120,121],"deep learning","Japanese","A walkthrough of a Japanese-English machine translation model",[22],"_Note_: this is a walkthrough of the JP-EN translation application that I'm no longer hosting online to save on cloud hosting fees. I assume the reader understands the main principles of deep learning and related concepts such as gradient descent and loss functions.\n\nWhen deciding on a topic for my Springboard capstone project, I knew quickly that I wanted to work on a neural machine translation (NMT) model. Machine translation has been a focus of my studies within machine learning due to my facination with linguistics and learning foreign languages. I was drawn to work on a Japanese-English translator because of my familiarity with the language (I minored in Japanese in college and continue studying it in my free time) as well as curiosity to see how it would be to work with a language that does not use the Roman alphabet.\n\nAfter several months of work I'm happy to share the progress I've made so far with this project. In this post I'll detail various aspects of the project from data gathering and cleaning to deployment, and I'll also give a walkthrough of the neural network architecture and ideas for future improvements to the project.\n\n# The Data\n\nThe dataset I chose to use for this project was the Japanese-English Subtitle Corpus, or [JESC](https://nlp.stanford.edu/projects/jesc/). This is a corpus of 2.8 million pairs of sentences taken from internet crawls of television and movie subtitles. Unlike most translation datasets, the JESC features a large amount of slang and colloquialisms which was part of its draw for me. It also did not require a large amount of processing to get it into a Python-friendly format as it came from a simple text file (as opposed to more complicated formats such as an XML document).\n\nPreprocessing the data before feeding it to the neural network did require significant effort, however. This was largely due to some of the characteristics of Japanese text and how it gets encoded by computers. Let's go through a short description of written Japanese to get a feel for its differences from written English.\n\n## A brief overview of written Japanese\n\nWhile English sentences are typically in the subject-verb-object order (“I go to the store”), Japanese is subject-object-verb (“I to the store go”). The written language uses 3 different alphabets:\n\n1. Chinese characters called “kanji” (漢字) that denote ideas and thus are used to represent any words and can have multiple readings or pronunciations.\n\n2. A native syllabary (meaning the characters represent whole syllables instead of individual sounds) “hiragana” (ひらがな), which is characterized by fewer lines per character than kanji and with more curved shapes. Hiragana is used for conjugation (often following kanji that form the first part of a word), grammatical parts-of-speech like adverbs and conjunctions, native words not represented by kanji, and several other purposes.\n\n3. A second native syllabary “katakana” (カタカナ), characterized by few lines similar to hiragana, but with sharper lines and angles. Katakana is used for non-East Asian proper nouns and loan words, emphasizing text (like italic or bold letters in English), animal and plant names, and other uses.\n\n## Text Preprocessing\n\nIt turns out that because Japanese has multiple types of characters, native speakers can often use this feature to visually distinguish a sentence's individual words instead of using spaces as European languages use. This sentence, “I came from the supermarket”, features all three alphabets:\n\n私はスーパーから来た。\n\nWhile this visual division works fine for human readers of Japanese, it's still easier for software if it can break up English and Japanese text in similar ways, a process known in natural language processing as “tokenization”. Therefore we need a way to add spaces between logical portions of Japanese text, which can present challenges for things like compound words. For my project I used the popular NLP Python library [Spacy](https://spacy.io/) along with the Japanese tokenizer library [SudachiPy](https://github.com/WorksApplications/SudachiPy). This library actually has settings that allow you to vary the level of granularity in its tokenization. For instance, here is an example from the documentation showing the various options available to break up the word for “government official” (I opted for the middle-of-the-road option “B” in my project):\n\n```python\nmode = tokenizer.Tokenizer.SplitMode.C\n[m.surface() for m in tokenizer_obj.tokenize(\"国家公務員\", mode)]\n# -> ['国家公務員']\n\nmode = tokenizer.Tokenizer.SplitMode.B\n[m.surface() for m in tokenizer_obj.tokenize(\"国家公務員\", mode)]\n# -> ['国家', '公務員']\n\nmode = tokenizer.Tokenizer.SplitMode.A\n[m.surface() for m in tokenizer_obj.tokenize(\"国家公務員\", mode)]\n# -> ['国家', '公務', '員']\n```\n\nAnother issue is that while Japanese has some obviously different puncuation marks (periods are “。”, for example), even punctuation that looks similar to the English counterparts (！or !) have different unicode values. Handling these as well as removing potential accented characters from the English text (i.e., turning “résumé” into “resume”) and transforming the text of both languages from Unicode into ASCII encodings was the majority of my pre-processing and text normalization work. Additionally, each sentence needs to start and end with special tokens (literally `\u003Cstart>` and `\u003Cend>` in this case) as guidance for parsing each sentence.\n\n## Preparing The Text Data for Training\n\nSince neural networks can only work with numbers, it's necessary to convert text data into numerical data for both the input language and target (or “output”) language of a NMT model. This process gives a numerical value to each unique word that appears in the data along with “word indexes” to be able to look up the words by their numerical values and vice-versa. The process also creates vectors with the same length as the longest sentence in the dataset (one for the input sentences and one for the target sentences). Each sentence is then one-hot encoded as a vector with numerical values at the space of each word and zeroes padding the vector to the maximum sentence length.\n\nAfter this vectorization of the sentences has been performed, I split the data into the following ratios using Scikit-learn's `train_test_split` method: training data = 75%, validation data = 15%, test data = 10%. I then created a dataset using Tensorflow's `tf.data.Dataset` class.\n\n# Neural Network Architecture\n\nThe neural network used in this project was built using the [Tensorflow](https://www.tensorflow.org/) framework, and the architecture I chose for this project was a sequence-to-sequence (seq2seq) model, more specifically an encoder-decoder model with attention. Seq2seq models use recurrent-neural-networks (RNNs), a natural choice for NLP problems as RNNs are capable of working with variable-length inputs and thus can handle text of various lengths. An additional appeal of RNNs, especially in NLP contexts, is that they have a sense of “memory” by being able to pass state between the layers of a neural network.\n\n![encoder-decoder](/static/images/Encoder-Decoder-Model-for-Text-Translation.png)\n\n> _Depiction of a sequence-to-sequence model, \"EOS\" is \"end of sentence\" (Sutskever et al., 2014)_\n\nRNNs feature several sub-classes of networks such as long-short term memory networks (LSTMs). It turns out that standard RNNs have a difficult time if the space between references gets too long (an example would be having the context from the beginning of a long sentence to help translate the end of a sentence). This problem is referred to as the “long-term dependency problem”, and LSTMs were designed specifically to avoid this issue! They feature a concept called “cell state” that lets information pass between modules of a neural network to overcome the long-term dependency problem. My model specifically uses GRUs, or “Gated Recurrent Units” which, among other changes, simplifies the storing and passing around of this hidden state versus traditional LSTMs.\n\n![lstm-network-cell](/static/images/LSTM-network-cell.png)\n\n> _Visualization of an LSTM network cell ([Source](https://colah.github.io/posts/2015-08-Understanding-LSTMs/))_\n\nAt the layer of the encoder-decoder part of the architecture, the encoder RNN reads in an input sentence and creates a vector representation of that sentence, which is then used by the decoder to extract a target sequence of that vector. During this process each word in the input sentence is assigned a weight value by the attention mechanism (described below), and this weight value is in turn used by the decoder to predict the next word in the target sentence.\n\n## Loss Function\n\nThis model used the sparse categorical cross-entropy method. Cross-entropy loss methods are also called “log-loss” methods and use a logarithmic function to increase the loss value as the predicted probability of an observation diverges from the actual observation. Sparse categorical cross-entropy is a specific use case for lots of categories (predicted translated words, in our case) that are represented as one-hot encodings, i.e. as vectors with a few ones and many zeros.\n\n## Attention Mechanism\n\nThe general idea of an attention mechanism is to prevent the model from associating a single vector with each sentence and use “attention weights” to influence which input vectors to prioritize and create a “context vector” that summarizes the input data received by the decoder. In encoder-decoder LSTMs without this attention mechanism, only the last hidden state value would be used to generate the context vector.\n\nThe network used in this project utilized Bahdanau attention, which works by using a linear combination of the encoder and decoder states to generate the context vector. The context vectors created by the Bahdanau attention mechanism as well as previously generated target words are then used by the model to predict a new target word. See the original paper that introduced Bahdanau attention, [\"Neural Machine Translation by Jointly Learning to Align and Translate\"](https://arxiv.org/pdf/1409.0473.pdf), for more information.\n\n![bahdanau-attention](/static/images/attention_bahdanau.png)\n\n> _Bahdanau Attention Diagram (Bahdanau et al., 2015)_\n\n## Optimizer\n\nThe optimization algorithm used in this model is the Adam optimizer. This optimizer is an extention of the stochastic gradient descent algorithm, and improves on it by using an adaptive learning rate to speed up optimization. More technically, the Adam algorithm calculates the exponential moving average of the gradient and squared gradient and uses two parameters (beta1 and beta2) to control the decay rate of these two averages. The original paper that introduced the Adam optimizer can be found here: [\"Adam: A Method for Stochastic Optimization\"](https://arxiv.org/pdf/1412.6980.pdf).\n\n# Model Performance\n\nThis model was trained in a Jupyter notebook with an accessible GPU hosted by [Paperspace](https://www.paperspace.com/) (the notebook can be found here: [Encoder-Decoder model for Japanese-to-English Translation](https://console.paperspace.com/te1qh03hs/notebook/prqqoypka)). After 100 epochs that ran for about 30 hours and another 12 hours for evaluating the model on test data, the process was complete.\n\nI evaluated the models using the METEOR metric. Unfortunately, the performance was not as great as I had hoped (it rarely is!). The average METEOR score per translation turned out to be just 0.10576, however this doesn't mean that roughly 1 in 10 words matched the expected translation. Briefly, the METEOR metric calculates the harmonic mean of unigram recall and precision together while giving more weight to recall. See my post on evaluating machine translation models here for more details: [\"How to Evaluate A Machine Translation Model\"](/blog/evaluating-machine-translation-models).\n\n# Deployment\n\nOnce training and evaluation were complete, I chose to deploy this model through this site (though as of 2023 I am no longer hosting it online). After saving the weights of the model using Tensorflow's own model saving method and saving the input and target language vectorizers as pickle objects, I built a class within a [Django](https://www.djangoproject.com/) backend that instantiates a model, uses the saved, static weights and vectorizers from the Paperspace training, and returns a predicted English translation. The entire Django application runs within a [Docker](https://www.docker.com/) container.\n\n![deployment-architecture](/static/images/jp-en-translation-deployment.png)\n\n> _Data flow in the deployed application_\n\nThe application has basic error handling in place on both the frontend and backend, and logging on the backend as well.\n\n# Ideas for Further Development\n\nEven though I was not satisfied with the model performance I remembered the maxim that “perfect is the enemy of done” and decided to continue with deploying the model, as completing an end-to-end machine learning project was more important than chasing a high metric score for too long. Nevertheless I'd like to share potential ideas for improving the model's performance and additional deployment ideas.\n\n## Data\n\nData is the fuel of machine learning models, and one of the first steps I intend to take when returning to this project is adding additional training data. While I thought it could be an interesting feature at the time, in retrospect having a dataset like the JESC that features a lot of slang hampers the model performance from a user standpoint, and it tends to be missing a lot of common vocabulary. Widely-used translation apps like [Google Translate](https://translate.google.com/) deal mostly with formal written language, and since translation apps have set a paradigm of avoid informal speech and text I would seek out additional datasets with more standard vocabulary. I would expect having additional training and testing data like this to have the largest impact on model performance.\n\nAnother idea is changes to the data preprocessing methods, especially choosing different options for how the Japanese and English text is tokenized as I described earlier. However, I would expect these changes to have less significant affects on the score versus changes to the data and model architecture.\n\n## Architecture and Hyperparameters\n\nWhile using an encoder-decoder with LSTMs/GNUs are the dominant architecture choice for a machine translation model, some changes to the architecture are worth exploring. The initial change I would make would be to use standard LSTMs instead of GNUs within the encoder and decoder, as LSTMs have been shown to have better performance than GNUs for some neural machine translation cases.\n\nAnother change would be to the attention mechanism used. While I used the Bahdanau attention mechanism here I could also explore using Luong attention, which is a newer method that attempts to improve on Bahdanau attention by simplifying the calculations of attention scores, leading to a faster and more efficient attention mechanism. It also uses the decoder's current hidden state to create the context vector, unlike the Bahdanau's use of the previously computed hidden state.\n\nA more significant change than these architecture adjustments I've discussed so far would be to use a model utilizing transfer learning with [BERT](https://github.com/google-research/bert), or “Bidirectional Encoder Representations from Transformers”. Transfer learning is a technique wherein neural networks trained for one task are used for predictions on a different (but usually related) task. Transformers are a class of neural networks with the encoder-decoder pattern that deal with long-term dependencies even better than LSTMs, and BERT is a class of transformer models that show excellent performance on a variety of NLP tasks than can be used with several pretrained models. Comparing my more “from scratch” model described in this post versus a transfer-learning-with-BERT model would be a very interesting follow up to this project.\n\nChanging the hyperparameters could also have a significant impact on performance. However, because the size of the GPU memory limited how much data I could process at a time, the primary hyperparameter I could significantly adjust would be the total number of epochs. Nonetheless there's always the chance that training on the same data for longer could lead to overfitting, and given that training for 100 epochs took well over a day, fine-tuning this value could take a significant amount of time, assuming I'm limited to working with one GPU.\n\n## Deployment\n\nThe deployment is generally working fine, although one thing I would like to add is a caching layer to avoid recomputing estimated translations for common input values. For this I would probably use [Redis](https://redis.io/) or a similar in-memory database to store common input-output pairs of sentences.\n\nIf usage of the app grew large enough, considering strategies for deploying, managing, and networking multiple instances of the backend application would be fun. Using [Kubernetes](https://kubernetes.io/) to orchestrate multiple Docker containers would be a natural choice here, and my existing [NGINX](https://www.nginx.com/) configuration could be updated to handle routing requests to multiple backend servers.\n\n# Extra Lessons Learned\n\nI greatly enjoyed working on this project, and I learned a great deal during the process. Often when I read other project walkthroughs I wish developers would share some “war stories” of unexpected hurdles, both to avoid giving readers the impression that development is very rarely a smooth process and to help readers avoid similar pitfalls. Accordingly, here are a two interesting issues I didn't expect to run into.\n\nSo much of the material I've read about machine learning and data science says that you'll spend much more time on data cleaning and preprocessing than you expect, and that was true for me as well. One particular error I dealt with that stood out was the appearance of two line separator unicode ([U+2028](https://www.fileformat.info/info/unicode/char/2028/index.htm)) characters (which I had never encountered until now) hiding in 2 Japanese sentences, which didn't show up until I saved the processed data to local files. When I loaded these files the Japanese sentences would have 2 extra instances versus the English sentences because of these line separators, giving me an “off by 2” error amongst 2.8 million sentences!\n\nAnother tool's quirks that I noticed which will save me lost time from now on is some of the nuances of working with the Large File Storage Git extension, aka git-lfs or \"[LFS](https://git-lfs.github.com/)\". LFS allows you to store large files on a site like Github using pointer files while the large file itself is stored on a different server. Frequently when I would clone my project from one environment to another, say from my local computer to Paperspace, I unconciously thought the large files themselves would be included in the cloning process, but I shouldn't have been surprised in retrospect that only the pointer file would move while I would have to manually copy/paste the large files themselves!\n\n# Acknowledgements\n\nI want to give a huge thank you to my Springboard mentor, Dhiraj Kumar. During our weekly calls (which were late in the evening for his timezone!) he would regularly provide me with suggestions and feedback that kept me moving forward when I could have gotten stuck or spent too much time on a particular aspect or another of the project. Please check out his [Medium blog](https://dhirajkumarblog.medium.com/) and [YouTube channel](https://www.youtube.com/channel/UCuOT2b1Umrr0MittMzuxNcA/).","src/content/blog/jp-en-translator-walkthrough.mdx","e81e8021b31c0f66","jp-en-translator-walkthrough.mdx","launching-mitchell-technology-consulting",{"id":128,"data":130,"body":136,"filePath":137,"digest":138,"legacyId":139,"deferredRender":27},{"title":131,"date":132,"tags":133,"draft":19,"summary":134,"authors":135},"Launching Mitchell Technology Consulting","2025-9-4",[18,17],"Announcing my new freelancing business",[22],"We constantly hear about companies and institutions struggling with data breaches and privacy concerns. But the same data we generate massive amounts of every day present immense potential for positive change. To improve how organizations protect this data and safely use it in data-intensive applications like artificial intelligence, I'm launching a freelancing business, [Mitchell Technology Consulting](https://mitchell-technology-consulting.com/).\n\nI've spent over 15 years working at a wide range of organizations, from early-stage startups to large government institutions. I've utilized multiple technologies building and maintaining many types of data-focused products. I've succeeded both as an individual contributor and team lead, and I'm comfortable working with and presenting to any section of an organization.\n\nI believe Mitchell Technology Consulting[^1] is the perfect way for me to work with a wide range of businesses to improve data utility and privacy in AI and other software applications. And I'm especially excited about the possibility of expanding the usage of privacy-enhancing technologies such as differential privacy – a method that allows us to analyze data without revealing sensitive individual information.\n\nIf you want to improve the way your team uses and protects your organization's data and the applications that use that data, [schedule a free consultation](https://mitchell-technology-consulting.com/forms/contact/) today!\n[^1]: I can't believe this wasn't already a registered business name in California 😄","src/content/blog/launching-mitchell-technology-consulting.mdx","c503a76369763c2e","launching-mitchell-technology-consulting.mdx","python-features-in-js",{"id":140,"data":142,"body":148,"filePath":149,"digest":150,"legacyId":151,"deferredRender":27},{"title":143,"date":144,"tags":145,"draft":19,"summary":146,"authors":147},"Three Python Features I Would Love To Have In JavaScript","2020-07-10",[37,64],"Python syntax and features I wish the ECMA International group would add to JavaScript",[22],"As someone who primarily learned to code using JavaScript, reading languages like C and Java wasn't too much of a struggle once I learned to read the typing-related code (something that became all the more easy after adopting TypeScript). But once I started digging deeper into machine learning and data science it became clear I would not be able to avoid learning Python. I was reluctant to learn it primarily because its syntax is so different from that of JavaScript (whitespace?!), and I was unmoved by people and [comics](https://xkcd.com/353/) singing the language's praises (well, maybe I was slightly moved by the comics).\n\nBut I eventually acquiesced and once I got comfortable reading and writing Python I discovered some things I actually enjoyed about the language. In fact these were things I wish I could adopt into my JavaScript code. Below is a short list of these features:\n\n# 1. Slicing Notation\n\nThis was probably the first part of Python's syntax that made me react with “Okay, that's a pretty nice feature.” Python's slicing syntax gives you the ability to easily get multiple subsections of any list (i.e., “array” in JavaScript). It looks like the following:\n\n```python\nexample_list = [1, 2, 3, 4]\nexample_list[:1] # -> [1]\nexample_list[1:] # -> [2, 3, 4]\nexample_list[1:3] # -> [2, 3]\n```\n\nAs you can see, the slice notation syntax can be thought of as consisting of optional \"start\" and \"stop\" values like so `[start:stop]` (Technically both the \"start\" and \"stop\" values can be optional because using `[:]` will return a complete copy of the original list), but you can also use a \"step\" property to skip values within the sliced subsets like so:\n\n```python\nexample_list[0:4:2] # -> [1, 3]\n```\n\n# 2. List Comprehensions\n\nList comprehensions are a prime example of one of the original goals of Python: to make code read more like a human language (at least if you're an English speaker!). Comprehensions give you a simple way to create new lists using this basic syntax:\n\n```python\n[expression for value in iterable]\n```\n\nHere is a simple example:\n\n```python\ndoubled_list = [i * 2 for i in [1,2,3]]\ndoubled_list # -> [2, 4, 6]\n```\n\nYou can even create nested comprehensions, which are especially useful when dealing with nested data structures such as matrices:\n\n```python\noriginal_matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ntransposed_matrix = [[row[i] for row in original_matrix] for i in range(3)]\ntransposed_matrix # [[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n```\n\nUnfortunately, although list comprehensions were actually on the roadmap for ECMAScript 2015 and even implemented in some versions of Firefox, the feature was later removed: [Array comprehensions | MDN](http://www-lia.deis.unibo.it/materiale/JS/developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Array_comprehensions.html) (12/2021 update: comprehensions are being reconsidered for ECMAScript 7!).\n\n# 3. Named Parameters\n\nPython gives you the ability to both pass arguments to a method in a set order, just like you would in JavaScript, or create named parameters that are defined with a default value and that can then be passed in any order. Let's look at an example:\n\n```python\ndef rectangle_area(width=0, height=0):\n    return width * height\n```\n\nI could call this method in any of the following ways:\n\n```python\nrectangle_area(5, 7)\nrectangle_area(width=5, height=7)\nrectangle_area(height=7, width=5)\n```\n\nYou can sort of adopt this pattern in JavaScript methods using objects:\n\n```javascript\nfunction rectangleArea(rectangleObj) {\n  return rectangleObj.width * rectangleObj.height\n}\nrectangleArea({ width: w, height: h })\nrectangleArea({ height: h, width: w })\n```\n\nBut I personally find the Python pattern easier to read at a glance, especially for methods with more than three or four arguments which are common in several data science and machine learning libraries.\n\nIf you're a developer who has worked with JavaScript but never with Python, hopefully this gives you a small taste of some of the things Python does well. Some features such as the mandatory whitespace and lack of keywords for variable declarations still look a little strange when I step back into Python after working in JavaScript for a while, but I am enjoying working with these features and many other methods in the standard library on a daily basis.","src/content/blog/python-features-in-js.mdx","4a46ea7dc52caadc","python-features-in-js.mdx","software-engineer-interview-questions",{"id":152,"data":154,"body":161,"filePath":162,"digest":163,"legacyId":164,"deferredRender":27},{"title":155,"date":156,"tags":157,"draft":19,"summary":159,"authors":160},"Interview Questions I Ask of Teams as a Software Engineering Candidate","2024-01-17",[158,18],"interviewing","Insightful questions to ask at the end of your interview",[22],"Several years ago at a startup I was working at, I was interviewing a candidate for an open role as a fellow software engineer. The candidate was a little nervous, which I didn't count against them, but at the end of my time slot I asked them if they had any questions for me, and the candidate responded with “no, I think I'm good.” This raises a big yellow flag as an interviewer: even if it isn't the intention, a lack of questions can often indicate that you aren't that interested in the role, the team, or the company as a whole. I feel that interviewees should always take advantage of this often too-brief amount of time in interviews to learn all they can about a team and company with which they may be spending a significant part of their lives for the next few years or possibly decades. In this post I'd like to share the set of questions I always try to ask teams I interview with.\n\nBefore jumping into the content, I realize that sometimes a person can be in a desperate situation to find a new role and may be willing to take almost any offer that is extended to them. Trust me when I say I've been there: I've been laid off, I've struggled to escape a bad manager, and I've had successive rounds of bad interviews that killed my confidence. I was once even asked to leave an interview early because I was bombing the technical questions so badly. However, I think that even in those types of situations that it's nevertheless worth asking these kinds of questions because you'll at least have a better idea of what to expect. It's also worth asking many of these questions to multiple colleagues and managers, because if you receive wildly different answers it can indicate mismatched expectations or broken communication between managers and individual contributors.\n\nI'll list out each question I like to ask by category, and then describe what type of insight the question should give you about the team/organization you're interviewing with.\n\n## Questions about Work Assignments\n\n* _Can you describe how work is typically assigned?_\n\nThis should give you an indication of how collaborative the work environment is. If you're early in your career it may be expected to be assigned tasks without much input, but as you become more senior it can be demoralizing to have little or no say in your assignments. It will also prevent you from growing as an engineer from one who can complete a work ticket to one who can talk to users and customers to figure out their needs and begin architecting solutions in a team setting.\n\n* _How do you work with stakeholders and project managers to design or iterate on requirements?_\n\nThis is similar to the above question, with more emphasis placed on iterative development and (hopefully) an answer will describe where you would fit within the development of features and products.\n\n## Questions about Fires 🔥\n\n* _Can you tell me about the last team emergency you had, like a very unhappy customer or broken deployment, and how the team overcame it?_\n\nIt's all fine to learn how a team works on a normal day, but what about when things go really wrong? These are the types of days you'll most strongly remember, for better or for worse. How managers and teammates respond on these types of days can instill loyalty and comraderie for strong teams, or bitterness and resentment for weak ones.\n\n* _How does the team learn from mistakes like these and institute changes so that mistakes aren't repeated?_\n\nIt's possible that in a young organization (like an early-stage startup) that processes such as post-mortems have not yet been instituted, or that no emergencies have been encountered. In that case, I typically like to create a hypothetical scenario such as in the previous question and ask how they _might_ respond.\n\n## Skills and Career Development\n\n* _How does the team support career development (conferences, continuing education, etc.)?_\n\nSimilar to the fires question above, early-stage startups rarely have the resources to support much career development. But hopefully in lieu of money for conferences or classes they can provide time for hackathons or try to provide formal or informal mentoring. Leaders who haven't thought about this kind of thing at all may present a yellow or red flag as an organization that will burn you out with constant work with few new skills or certifications to show for it when you start searching for your next opportunity.\n\n* _How do your performance reviews work?_\n\nThis question should give you a clear insight into how the organization thinks about career development and promotions, and how you'll be expected to report on your work.\n\n## Role Expectations\n\n* _If a candidate were to accept this offer and it was 6-12 months from now, what would you imagine a wildly successful candidate to have done in that time?_\n\nThis is an excellent question to end on. It can create a sense of excitement for both parties and often give you insight into what some unwritten expectations are and what the organization deeply values in an employee. If you get an offer and can execute on the answer to this question, you'll likely be on the fast track to recognitions of your hard work such as bonuses and promotions.\n\nThis isn't an exhaustive list, and obviously there may be questions you can ask that are more specific to the role or organization. But this should give you some ideas about how to get the most out of your Q&A time as a candidate, and hopefully at the end of the process you'll have a clearer picture of the team that you are considering joining.\n\nBest of luck!","src/content/blog/software-engineer-interview-questions.mdx","b8de92e7d7346931","software-engineer-interview-questions.mdx","sql-order-of-execution",{"id":165,"data":167,"body":174,"filePath":175,"digest":176,"legacyId":177,"deferredRender":27},{"title":168,"date":169,"tags":170,"draft":19,"summary":172,"authors":173},"SQL's Order of Execution","2020-12-16",[171],"sql","Details on how SQL optimizes database queries",[22],"If you started learning SQL after having learned a language like Java or Python, it takes getting used to different concepts inherent in database querying, let alone the varied differences in the way SQL runs. Once you've started writing anything more complicated than the “Hello, World!” of SQL queries, meaning `SELECT * FROM TABLE;`, then you've probably encountered errors that hint at one of the more unintuitive aspects of the language: the order of execution in a SQL statement.\n\nAs an example, say we're creating a derived column and then we want to group the results by that new column. In this code example we'll create a new column summing the value of multiple customer contracts from a hypothetical database:\n\n```sql\nSELECT\n    customer_id,\n    contract_1,\n    contract_2,\n    contract_3,\n    SUM(contract_1, contract_2, contract_3) AS contract_totals\n  FROM contract_table;\n```\n\nIn the case where you want to order by the size of the contract totals, you would likely encounter an error stating that the column `contract_totals` does not exist:\n\n```sql\nSELECT\n    customer_id,\n    contract_1,\n    contract_2,\n    contract_3,\n    SUM(contract_1, contract_2, contract_3) AS contract_totals\n  FROM contract_table\n  ORDER BY contract_totals;\n```\n\nIt turns out that standard SQL runtimes will attempt to run a “group by” statement before the “select” statement, and in fact the “select” statement is one of the last parts of a SQL query that gets executed. The following is the order that a typical SQL statement is executed in with a brief description of that command:\n\n1. `FROM`: pick the table(s) to be queried\n2. `WHERE`: filter the rows\n3. `GROUP BY`: aggregate the rows\n4. `HAVING`: filter the aggregated rows\n5. `SELECT`: select the columns that appear in the output\n6. `ORDER BY`: sort rows by value(s)\n7. `LIMIT`: restrict the maximum number of returned rows\n\nUnfortunately there's not a great way to work around this process and your best bet is to have this order of execution memorized. For instance, returning to our previous example with the contract amount summation, it's easiest to repeat the sum expression within the `ORDER BY` statement:\n\n```sql\nSELECT\n    customer_id,\n    contract_1,\n    contract_2,\n    contract_3,\n    SUM(contract_1, contract_2, contract_3) AS contract_totals\n  FROM contract_table\n  ORDER BY sum(contract_1, contract_2, contract_3);\n```\n\nEven though this should set off every programmer's DRY alarm (DRY=“don't repeat yourself”), writing your SQL queries with the execution order in mind will reduce the number of errors you encounter, especially as you transition from a SQL newbie to a seasoned user!","src/content/blog/sql-order-of-execution.mdx","bd824a0ee05f1cc8","sql-order-of-execution.mdx","updated-website",{"id":178,"data":180,"body":187,"filePath":188,"digest":189,"legacyId":190,"deferredRender":27},{"title":181,"date":182,"tags":183,"draft":19,"summary":185,"authors":186},"Updated and Improved Website","2023-07-16",[63,65,184],"vercel","Migrating the website to Next.js and Vercel",[22],"I've finally completed a major renovation of this website, after starting this work\nalmost 2 years ago. Better late than never! 😅\n\nI hit a few roadblocks in trying to incrementally update my existing website's frontend codebase from a standard [React](https://react.dev/)- and [TypeScript](https://www.typescriptlang.org/)-based app to a modern [Next.js](https://nextjs.org/)-based application, and more than once I stepped away from this work for a few months. Probably the most difficult challenges included trying to update several [Material UI](https://mui.com/) components between major versions of that library, including some API changes that made the site's responsive design break in sometimes subtle ways.\n\nUltimately I decided to rebuild the site using a [blog template](https://github.com/timlrx/tailwind-nextjs-starter-blog) I found on the Next.js site. The template includes integrations with the [Tailwind CSS](https://tailwindcss.com/) framework and some easy configurations for email and site analytics. I also appreciate that the template has built-in support for both dark and light themes, and the usage of the great [MDX](https://mdxjs.com/) tool makes writing new posts much easier. On the previous version of the site I was manually encoding the HTML and JSX around the text of each post! Additionally, Vercel and Next.js apps include great performance when it comes to things like page loading and accessibility, as verified by high scores using Google's [Lighthouse](https://developer.chrome.com/docs/lighthouse/overview/) developer tool, as well as an A+ [SSL report](https://www.ssllabs.com/ssltest/analyze.html?d=curt-mitch.com) from Qualys.\n\nAnd speaking of making updates easier, I've migrated the cloud hosting platform for this site from [DigitalOcean](https://www.digitalocean.com/) to [Vercel](https://vercel.com) (probably not too big of a surprise since Next.js is a Vercel-supported framework). In order to make updates to the previous version of the site, I would have to SSH into my DigitalOcean server, do a git pull on the website repository, and then rebuild new Docker images before launching the containers (I also used Docker's image registry to skip the git pull and image build steps). Using Vercel's [serverless](https://www.redhat.com/en/topics/cloud-native-apps/what-is-serverless) architecture and support in Github Actions makes writing a new post much easier; essentially I just need to create a new markdown file (and upload any associated images), and then push these to the deployment branch on Github. Greatly reducing the friction to updating the site will it make that much easier to write new content. One cost of this migration is the Django backend I hosted on DigitalOcean that allowed me to support APIs for my Japanese-English translation and Urdu numeral-recognition systems, but I may try to find a way to restore support for these in the future. As is, they were used very little and their large sizes meant I was paying more than $20 per month in cloud usage fees.\n\nI don't mean for this to sound like a knock on DigitalOcean. On the contrary, it is a great platform that helped me learn some dev ops basics such as setting up and maintaining a Linux box for deploying a web application, as well as some more complicated processes such as configuring [NGINX](https://nginx.com), [Let's Encrypt](https://letsencrypt.org), and other services to enable a modern web application. They also have some of the best online [documentation and tutorials](https://docs.digitalocean.com) for setting up and maintaining cloud applications and dependencies. They'll continue to be one of my top choices for cloud providers in future projects. But it is hard to beat the convenience of a serverless architecture for a personal blog, and especially when that includes Vercel's integrations and support for a Next.js codebase hosted on Github that has a generous [free tier](https://vercel.com/pricing) for personal projects.\n\nLastly, I also took some time with this refactor to do a little branding. The [golden ratio](https://en.wikipedia.org/wiki/Golden_ratio) (φ) is one of my favorite mathematical constants, so I decided to make an SVG of the golden rectangle (a rectangle with side length proportions are equal to the golden ratio) based on an [image](https://commons.wikimedia.org/wiki/File:Fibonacci_spiral.svg) on Wikimedia that's in the public domain. Relatedly, I thought to give the website a name rather than just “Curt-Mitch.com.” Playing off of the golden ratio idea, I thought “A Marvelous Balance” seemed like a good name. It feels like an elegant title for a website that contains writings on multiple interrelated topics.\n\nIn any case, I'm looking forward to writing interesting content about a variety of topics on a more consistent cadence. If you'd like to, check out the repository for this site here: [https://github.com/curt-mitch/curt-mitch.com/](https://github.com/curt-mitch/curt-mitch.com/).\n\nThank you for reading!","src/content/blog/updated-website.mdx","662d26e584b4fab0","updated-website.mdx","visual-intuition-differential-privacy-part1",{"id":191,"data":193,"body":200,"filePath":201,"digest":202,"legacyId":203,"deferredRender":27},{"title":194,"date":195,"tags":196,"draft":27,"summary":198,"authors":199},"Building a Visual Intuition for Differential Privacy, Part 1","2024-10-28",[197,79],"differential privacy","Teaching and Learning About Differential Privacy Through Visualizations",[22],"When I started working professionally the field of privacy-enhancing technologies (PETs), it didn't take me long to realize that there is a real derth of material about these technologies that are made for beginners. That could be because PETs incorporate a range of concepts and techniques from other fields such as cryptography and probability, or it could be that most university courses and books I come across online that teach PETs often include them as a small section with material that's more broadly about privacy, security, data analysis, or machine learning.\n\nEven though PETs do indeed consist of ideas and processes from many disparate fields(1), there are many useful analogies and visualizations that can be employed to provide some intuition for how they work. In this post I'd like to show some visual intuitions for differential privacy, one of the more mature and widely-used privacy-enhancing technologies.\n\n## What is Differential Privacy?\n\nIn short, differential privacy (DP) is a technique for extracting useful insight from data while providing just enough statistical noise in those values to maintain the privacy of entities within that data. Said in a slightly different way: differential privacy adds noise to data outputs that provide plausible deniability for data subjects.\n\nMore formally, when someone requests an output value that describes a dataset (such as a count or average of a set of values in that dataset), DP adds or subtracts a random number from the output value such that sees this altered version of the output can no longer determine if a specific person or entity is in the dataset. We can visualize this by looking at two rows of a fake payroll dataset and the sum of the salary values from those rows.\n\nOriginal table:\n\u003CExampleDatabaseTable />\n\ntable with row removed:\n\u003CExampleDatabaseTable removedRow={1}/>\n\n Let's imagine a privacy attacker does not have access to these tables directly, but has access to the two different salary sum values. Clearly this person can determine the value of the missing salary by simply subtracting the sums:\n\n$$\nsalary\\_sum_1 - salary\\_sum_2 = 254000 - 199000 = 55000\n$$\n\n* add random noise to output\n\n* what kind of random noise","src/content/blog/visual-intuition-differential-privacy-part1.mdx","43a851a5c9502977","visual-intuition-differential-privacy-part1.mdx","nested-route/introducing-multi-part-posts-with-nested-routing",{"id":204,"data":206,"body":214,"filePath":215,"digest":216,"legacyId":217,"deferredRender":27},{"title":207,"date":208,"tags":209,"draft":27,"summary":212,"authors":213},"Introducing Multi-part Posts with Nested Routing","2021-05-02",[210,65,211],"multi-author","feature","The blog template supports posts in nested sub-folders. This can be used to group posts of similar content e.g. a multi-part course. This post is itself an example of a nested route!",[22],"# Nested Routes\n\nThe blog template supports posts in nested sub-folders. This helps in organisation and can be used to group posts of similar content e.g. a multi-part series. This post is itself an example of a nested route! It's located in the `/data/blog/nested-route` folder.\n\n## How\n\nSimplify create multiple folders inside the main `/data/blog` folder and add your `.md`/`.mdx` files to them. You can even create something like `/data/blog/nested-route/deeply-nested-route/my-post.md`\n\nWe use Next.js catch all routes to handle the routing and path creations.\n\n## Use Cases\n\nHere are some reasons to use nested routes\n\n- More logical content organisation (blogs will still be displayed based on the created date)\n- Multi-part posts\n- Different sub-routes for each author\n- Internationalization (though it would be recommended to use [Next.js built-in i8n routing](https://nextjs.org/docs/advanced-features/i18n-routing))\n\n## Note\n\n- The previous/next post links at bottom of the template are currently sorted by date. One could explore modifying the template to refer the reader to the previous/next post in the series, rather than by date.","src/content/blog/nested-route/introducing-multi-part-posts-with-nested-routing.mdx","10fe6b739893c32f","nested-route/introducing-multi-part-posts-with-nested-routing.mdx"]